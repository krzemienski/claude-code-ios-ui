//
//  TranscriptionService.swift
//  ClaudeCodeUI
//
//  Transcription API service for audio to text conversion
//  REAL BACKEND - NO MOCKS
//

import Foundation
import AVFoundation
import Speech

// MARK: - Transcription Models

struct TranscriptionRequest: Codable {
    let audioData: Data
    let format: String // "wav", "m4a", "mp3"
    let language: String? // Optional language code
    let model: String? // Optional model selection
}

struct TranscriptionResponse: Codable {
    let text: String
    let confidence: Double?
    let language: String?
    let duration: Double?
    let segments: [TranscriptionSegment]?
}

struct TranscriptionSegment: Codable {
    let text: String
    let startTime: Double
    let endTime: Double
    let confidence: Double?
}

// MARK: - Transcription Service

final class TranscriptionService: NSObject {
    
    static let shared = TranscriptionService()
    
    private let apiClient = APIClient.shared
    private let baseURL = "http://localhost:3004"
    
    // Local speech recognition as fallback
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // Recording properties
    private var audioRecorder: AVAudioRecorder?
    private var recordingURL: URL?
    
    override init() {
        super.init()
        requestSpeechAuthorization()
    }
    
    // MARK: - Authorization
    
    private func requestSpeechAuthorization() {
        SFSpeechRecognizer.requestAuthorization { [weak self] authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    print("‚úÖ Speech recognition authorized")
                case .denied:
                    print("‚ùå Speech recognition denied")
                case .restricted:
                    print("‚ö†Ô∏è Speech recognition restricted")
                case .notDetermined:
                    print("‚ùì Speech recognition not determined")
                @unknown default:
                    print("Unknown speech recognition status")
                }
            }
        }
    }
    
    // MARK: - Recording
    
    func startRecording(completion: @escaping (Result<URL, Error>) -> Void) {
        // Setup audio session
        let audioSession = AVAudioSession.sharedInstance()
        
        do {
            try audioSession.setCategory(.playAndRecord, mode: .default)
            try audioSession.setActive(true)
            
            // Create recording URL
            let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            recordingURL = documentsPath.appendingPathComponent("recording_\(Date().timeIntervalSince1970).m4a")
            
            // Setup audio recorder
            let settings: [String: Any] = [
                AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
                AVSampleRateKey: 44100,
                AVNumberOfChannelsKey: 1,
                AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
            ]
            
            audioRecorder = try AVAudioRecorder(url: recordingURL!, settings: settings)
            audioRecorder?.delegate = self
            audioRecorder?.record()
            
            print("üéôÔ∏è Recording started at: \(recordingURL!.path)")
            completion(.success(recordingURL!))
            
        } catch {
            print("‚ùå Failed to start recording: \(error)")
            completion(.failure(error))
        }
    }
    
    func stopRecording() -> URL? {
        audioRecorder?.stop()
        audioRecorder = nil
        
        print("üõë Recording stopped")
        return recordingURL
    }
    
    // MARK: - Transcription via Backend
    
    func transcribeAudio(at url: URL) async throws -> TranscriptionResponse {
        print("üì§ Sending audio to backend for transcription...")
        
        // Read audio file
        let audioData = try Data(contentsOf: url)
        
        // Use APIClient for transcription
        let transcription = try await APIClient.shared.transcribeAudio(audioData: audioData, format: "m4a")
        
        print("‚úÖ Transcription successful: \(transcription.text.prefix(100))...")
        return transcription
    }
    
    // MARK: - Local Transcription (Fallback)
    
    func transcribeLocally(at url: URL) async throws -> String {
        print("üì± Using local speech recognition...")
        
        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            throw TranscriptionError.recognizerNotAvailable
        }
        
        return try await withCheckedThrowingContinuation { continuation in
            let request = SFSpeechURLRecognitionRequest(url: url)
            request.shouldReportPartialResults = false
            
            recognitionTask = recognizer.recognitionTask(with: request) { result, error in
                if let error = error {
                    continuation.resume(throwing: error)
                } else if let result = result, result.isFinal {
                    continuation.resume(returning: result.bestTranscription.formattedString)
                }
            }
        }
    }
    
    // MARK: - Live Transcription
    
    func startLiveTranscription(onPartialResult: @escaping (String) -> Void,
                               onFinalResult: @escaping (String) -> Void,
                               onError: @escaping (Error) -> Void) {
        
        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            onError(TranscriptionError.recognizerNotAvailable)
            return
        }
        
        do {
            // Configure audio session
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.record, mode: .measurement)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            
            // Create recognition request
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else {
                onError(TranscriptionError.recognitionRequestFailed)
                return
            }
            
            recognitionRequest.shouldReportPartialResults = true
            
            // Setup audio engine
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                recognitionRequest.append(buffer)
            }
            
            audioEngine.prepare()
            try audioEngine.start()
            
            print("üé§ Live transcription started")
            
            // Start recognition
            recognitionTask = recognizer.recognitionTask(with: recognitionRequest) { result, error in
                if let error = error {
                    onError(error)
                    self.stopLiveTranscription()
                } else if let result = result {
                    let transcription = result.bestTranscription.formattedString
                    
                    if result.isFinal {
                        onFinalResult(transcription)
                        self.stopLiveTranscription()
                    } else {
                        onPartialResult(transcription)
                    }
                }
            }
            
        } catch {
            onError(error)
        }
    }
    
    func stopLiveTranscription() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        
        recognitionRequest = nil
        recognitionTask = nil
        
        print("üõë Live transcription stopped")
    }
    
    // MARK: - Hybrid Transcription
    
    func transcribeWithFallback(at url: URL) async throws -> String {
        do {
            // Try backend first
            let response = try await transcribeAudio(at: url)
            return response.text
        } catch {
            print("‚ö†Ô∏è Backend transcription failed, falling back to local: \(error)")
            // Fallback to local transcription
            return try await transcribeLocally(at: url)
        }
    }
}

// MARK: - AVAudioRecorderDelegate

extension TranscriptionService: AVAudioRecorderDelegate {
    func audioRecorderDidFinishRecording(_ recorder: AVAudioRecorder, successfully flag: Bool) {
        print("Recording finished: \(flag ? "‚úÖ" : "‚ùå")")
    }
    
    func audioRecorderEncodeErrorDidOccur(_ recorder: AVAudioRecorder, error: Error?) {
        print("Recording error: \(error?.localizedDescription ?? "Unknown")")
    }
}

// MARK: - Errors

enum TranscriptionError: LocalizedError {
    case recognizerNotAvailable
    case recognitionRequestFailed
    case invalidResponse
    case backendError(Int)
    
    var errorDescription: String? {
        switch self {
        case .recognizerNotAvailable:
            return "Speech recognizer is not available"
        case .recognitionRequestFailed:
            return "Failed to create recognition request"
        case .invalidResponse:
            return "Invalid response from backend"
        case .backendError(let code):
            return "Backend error with status code: \(code)"
        }
    }
}

// Extension removed - functionality moved to APIClient.swift